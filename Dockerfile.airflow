# Enhanced Eurostat Airflow Dockerfile - OPTIMIZED FOR FAST BUILDS
# Optimized for the cleaned-up project with enhanced DAG and shared modules

FROM apache/airflow:3.0.1

# Set the USER to root temporarily to install system dependencies
USER root

# Install system dependencies for enhanced scripts (cached layer)
# - curl for SourceData.py data downloads
# - libpq-dev for psycopg2 (PostgreSQL connections)
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    libpq-dev \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Switch back to the airflow user
USER airflow

# OPTIMIZATION: Copy requirements FIRST for better layer caching
# This layer will only rebuild if requirements change
COPY requirements-airflow.txt /requirements.txt

# OPTIMIZATION: Install dependencies from requirements file
# Use Flask 3.x compatible versions of all dependencies
RUN pip install --no-cache-dir -r /requirements.txt

# OPTIMIZATION: Install Airflow providers separately (they're often already included)
RUN pip install --no-cache-dir --no-deps \
    apache-airflow-providers-postgres>=5.0.0 \
    apache-airflow-providers-common-sql>=1.0.0 \
    || echo "Providers already included in base image"

# Set Python path EARLY for better caching
ENV PYTHONPATH="${PYTHONPATH}:/opt/airflow/scripts:/opt/airflow/scripts/shared"

# Set environment variables for enhanced pipeline (cached layer)
ENV ENHANCED_PIPELINE_MODE=true
ENV USE_SHARED_MODULES=true
ENV AIRFLOW_VAR_USE_SHARED_HEALTH_DATASETS=true
ENV AIRFLOW_VAR_ENHANCED_BATCH_SIZE=2000
ENV AIRFLOW_VAR_ENABLE_STREAMING_LOAD=true

# Create necessary directories (cached layer)
RUN mkdir -p \
    /opt/airflow/temp_enhanced_downloads \
    /opt/airflow/logs/enhanced_pipeline \
    /opt/airflow/data_cache

# OPTIMIZATION: Copy scripts LAST (most frequently changed)
# This ensures script changes don't invalidate dependency cache
COPY ./scripts/ /opt/airflow/scripts/

# The enhanced DAG will be mounted via volumes in docker-compose-airflow.yaml
